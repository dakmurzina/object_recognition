{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PY2Faz4indQJ",
        "outputId": "cbcacc47-f5da-4a02-d5f6-8ea9c77bb9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_snippets\n",
            "  Downloading torch_snippets-0.499.25-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf\n",
            "  Downloading PyMuPDF-1.21.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (7.9.0)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.22.4)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (8.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.4.4)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (4.2.2)\n",
            "Requirement already satisfied: confection in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (0.0.4)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.5.28)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (3.7.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.1.1)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (6.0)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (2.4.6)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (0.4.0)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (4.5.0)\n",
            "Collecting lovely-tensors\n",
            "  Downloading lovely_tensors-0.1.14-py3-none-any.whl (16 kB)\n",
            "Collecting pre-commit\n",
            "  Downloading pre_commit-3.2.0-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.7/202.7 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.2.2)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (4.65.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from torch_snippets) (1.10.6)\n",
            "Collecting rich\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.16.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (2.0.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (4.7.0.72)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->torch_snippets) (2.25.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair->torch_snippets) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair->torch_snippets) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair->torch_snippets) (3.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair->torch_snippets) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torch_snippets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torch_snippets) (2.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fastcore->torch_snippets) (23.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (from fastcore->torch_snippets) (22.0.4)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (5.7.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->torch_snippets) (67.6.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines->torch_snippets) (22.2.0)\n",
            "Collecting lovely-numpy>=0.2.8\n",
            "  Downloading lovely_numpy-0.2.8-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from lovely-tensors->torch_snippets) (1.13.1+cu116)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (4.39.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->torch_snippets) (0.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->torch_snippets) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->torch_snippets) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->torch_snippets) (1.1.1)\n",
            "Collecting cfgv>=2.0.0\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting nodeenv>=0.11.1\n",
            "  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting virtualenv>=20.10.0\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting identify>=1.0.0\n",
            "  Downloading identify-2.5.21-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Levenshtein==0.20.9\n",
            "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_snippets) (3.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->torch_snippets) (3.15.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->torch_snippets) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (0.19.3)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->torch_snippets) (0.2.6)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2023.3.15)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (3.0)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.1.1)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.4.1 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair->torch_snippets) (2.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->torch_snippets) (0.7.0)\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26321 sha256=b9ddb4e8f88f259b07de752c55e3eed53c373a4218d4bb80c932af7992d85a81\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/17/1f/332799f975d1b2d7f9b3f33bbccf65031e794717d24432caee\n",
            "Successfully built typing\n",
            "Installing collected packages: fuzzywuzzy, distlib, xmltodict, virtualenv, typing, rapidfuzz, pymupdf, pygments, nodeenv, mdurl, loguru, jsonlines, jedi, identify, dill, cfgv, pre-commit, markdown-it-py, Levenshtein, rich, python-Levenshtein, lovely-numpy, lovely-tensors, torch_snippets\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "Successfully installed Levenshtein-0.20.9 cfgv-3.3.1 dill-0.3.6 distlib-0.3.6 fuzzywuzzy-0.18.0 identify-2.5.21 jedi-0.18.2 jsonlines-3.1.0 loguru-0.6.0 lovely-numpy-0.2.8 lovely-tensors-0.1.14 markdown-it-py-2.2.0 mdurl-0.1.2 nodeenv-1.7.0 pre-commit-3.2.0 pygments-2.14.0 pymupdf-1.21.1 python-Levenshtein-0.20.9 rapidfuzz-2.13.7 rich-13.3.2 torch_snippets-0.499.25 typing-3.7.4.3 virtualenv-20.21.0 xmltodict-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pygments",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch_snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qDqSpbU3XXwG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch_snippets\n",
        "\n",
        "import os\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZRRJu2eb1sU"
      },
      "outputs": [],
      "source": [
        "def xml_to_dict(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    return {'filename': xml_path,\n",
        "            \"image_width\": int(root.find(\"./size/width\").text),\n",
        "            \"image_height\": int(root.find(\"./size/height\").text),\n",
        "            \"image_channels\": int(root.find(\"./size/depth\").text),\n",
        "            \"label\": root.find(\"./object/name\").text,\n",
        "            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n",
        "            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n",
        "            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n",
        "            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icpms_dbe_NN"
      },
      "outputs": [],
      "source": [
        "# Convert human readable str label to int.\n",
        "label_dict = {\"keyboard\": 1, \"key\": 2, \"laptop\": 3, \"magnifying-glass\":4, \"mouse\":5, \"phone\":6 }\n",
        "# Convert label int to human readable str.\n",
        "reverse_label_dict = {1: \"keyboard\", 2: \"key\", 3: \"laptop\", 4: \"magnifying-glass\", 5: \"mouse\", 6: \"phone\"}\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms = None):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            root: str\n",
        "                Path to the data folder.\n",
        "            transforms: Compose or list\n",
        "                Torchvision image transformations.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.files = [image[:-4] for image in sorted(os.listdir(root)) if image[-4:]=='.jpg']\n",
        "        self.label_dict = label_dict\n",
        "    def __getitem__(self, i):\n",
        "        # Load image from the hard disc.\n",
        "        img = PIL.Image.open(os.path.join(self.root, self.files[i] + \".jpg\")).convert(\"RGB\")\n",
        "        # Load annotation file from the hard disc.\n",
        "        ann = xml_to_dict(os.path.join(self.root, self.files[i] + \".xml\"))\n",
        "        # The target is given as a dict.\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor([[ann[\"x1\"], \n",
        "                                            ann[\"y1\"], \n",
        "                                            ann[\"x2\"], \n",
        "                                            ann[\"y2\"]]], \n",
        "                                   dtype = torch.float32)\n",
        "        target[\"labels\"]=torch.as_tensor([label_dict[ann[\"label\"]]],\n",
        "                         dtype = torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor(i)\n",
        "        # Apply any transforms to the data if required.\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gmvXEHNhbl8"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms.transforms as T\n",
        "class Compose:\n",
        "    \"\"\"\n",
        "    Composes several torchvision image transforms \n",
        "    as a sequence of transformations.\n",
        "    Inputs\n",
        "        transforms: list\n",
        "            List of torchvision image transformations.\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms = []):\n",
        "        self.transforms = transforms\n",
        "    # __call__ sequentially performs the image transformations on\n",
        "    # the input image, and returns the augmented image.\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfY5rpL3hmLL"
      },
      "outputs": [],
      "source": [
        "class ToTensor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts a PIL image into a torch tensor.\n",
        "    Inputs\n",
        "        image: PIL Image\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        image = F.convert_image_dtype(image)\n",
        "        return image, target\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "    \"\"\"\n",
        "    Randomly flips an image horizontally.\n",
        "    Inputs\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                width, _ = F.get_image_size(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - \\\n",
        "                                     target[\"boxes\"][:, [2, 0]]\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whc1Cchzh5dt"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Transforms a PIL Image into a torch tensor, and performs\n",
        "    random horizontal flipping of the image if training a model.\n",
        "    Inputs\n",
        "        train: bool\n",
        "            Flag indicating whether model training will occur.\n",
        "    Returns\n",
        "        compose: Compose\n",
        "            Composition of image transforms.\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    # ToTensor is applied to all images.\n",
        "    transforms.append(ToTensor())\n",
        "    # The following transforms are applied only to the train set.\n",
        "    if train == True:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        # Other transforms can be added here later on.\n",
        "    return Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K1t2KSRiRkQ"
      },
      "outputs": [],
      "source": [
        "# Train dataset. \n",
        "# Set train = True to apply the training image transforms.\n",
        "train_ds = ImageDataset(\"./data/train\", get_transform(train = True))\n",
        "# Validation dataset.\n",
        "val_ds = ImageDataset(\"./data/val\", get_transform(train = False))\n",
        "# Test dataset.\n",
        "# test_ds = ImageDataset(\"./data/test2\", get_transform(train = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYwIdWC4kryg"
      },
      "outputs": [],
      "source": [
        "# Randomly shuffle all the data.\n",
        "indices = torch.randperm(len(train_ds)).tolist()\n",
        "# We split the entire data into 80/20 train-test splits. We further\n",
        "# split the train set into 80/20 train-validation splits. \n",
        "# Train dataset: 64% of the entire data, or 80% of 80%.\n",
        "train_ds = torch.utils.data.Subset(train_ds,\n",
        "           indices[:int(len(indices) * 0.64)])\n",
        "# Validation dataset: 16% of the entire data, or 20% of 80%.\n",
        "val_ds = torch.utils.data.Subset(val_ds, \n",
        "         indices[int(len(indices) * 0.64):int(len(indices) * 0.8)])\n",
        "# Test dataset: 20% of the entire data.\n",
        "# test_ds = torch.utils.data.Subset(test_ds, \n",
        "#           indices[int(len(indices) * 0.8):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh5SF2XNk110"
      },
      "outputs": [],
      "source": [
        "# Collate image-target pairs into a tuple.\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "# Create the DataLoaders from the Datasets. \n",
        "train_dl = torch.utils.data.DataLoader(train_ds, \n",
        "                                 batch_size = 4, \n",
        "                                 shuffle = True, \n",
        "                        collate_fn = collate_fn)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, \n",
        "                             batch_size = 4, \n",
        "                            shuffle = False, \n",
        "                    collate_fn = collate_fn)\n",
        "# test_dl = torch.utils.data.DataLoader(test_ds, \n",
        "#                                batch_size = 4, \n",
        "#                               shuffle = False, \n",
        "#                       collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt1wX435lEOQ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def get_object_detection_model(num_classes = 7, \n",
        "                               feature_extraction = True):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        num_classes: int\n",
        "            Number of classes to predict. Must include the \n",
        "            background which is class 0 by definition!\n",
        "        feature_extraction: bool\n",
        "            Flag indicating whether to freeze the pre-trained \n",
        "            weights. If set to True the pre-trained weights will be  \n",
        "            frozen and not be updated during.\n",
        "    Returns\n",
        "        model: FasterRCNN\n",
        "    \"\"\"\n",
        "    # Load the pretrained faster r-cnn model.\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "    # If True, the pre-trained weights will be frozen.\n",
        "    if feature_extraction == True:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "    # Replace the original 91 class top layer with a new layer\n",
        "    # tailored for num_classes.\n",
        "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats,\n",
        "                                                   num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeOQ1Q48lX38"
      },
      "outputs": [],
      "source": [
        "def unbatch(batch, device):\n",
        "    \"\"\"\n",
        "    Unbatches a batch of data from the Dataloader.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        X: list\n",
        "            List of images.\n",
        "        y: list\n",
        "            List of dictionaries.\n",
        "    \"\"\"\n",
        "    X, y = batch\n",
        "    X = [x.to(device) for x in X]\n",
        "    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
        "    return X, y\n",
        "def train_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Uses back propagation to train a model.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, losses\n",
        "@torch.no_grad()\n",
        "def validate_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Evaluates a model's loss value using validation data.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvp1ythylmfT"
      },
      "outputs": [],
      "source": [
        "def train_fasterrcnn(model, \n",
        "                 optimizer, \n",
        "                  n_epochs, \n",
        "              train_loader, \n",
        "        test_loader = None, \n",
        "                log = None, \n",
        "               keys = None, \n",
        "            device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Trains a FasterRCNN model using train and validation \n",
        "    Dataloaders over n_epochs. \n",
        "    Returns a Report on the training and validation losses.\n",
        "    Inputs\n",
        "        model: FasterRCNN\n",
        "        optimizer: torch optimizer\n",
        "        n_epochs: int\n",
        "            Number of epochs to train.\n",
        "        train_loader: DataLoader\n",
        "        test_loader: DataLoader\n",
        "        log: Record\n",
        "            torch_snippet Record to record training progress.\n",
        "        keys: list\n",
        "            List of strs containing the FasterRCNN loss names.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        log: Record\n",
        "            torch_snippet Record containing the training records.\n",
        "    \"\"\"\n",
        "    if log is None:\n",
        "        log = torch_snippets.Report(n_epochs)\n",
        "    if keys is None:\n",
        "        # FasterRCNN loss names.\n",
        "        keys = [\"loss_classifier\", \n",
        "                   \"loss_box_reg\", \n",
        "                \"loss_objectness\", \n",
        "               \"loss_rpn_box_reg\"]\n",
        "    model.to(device)\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        for ix, batch in enumerate(train_loader):\n",
        "            loss, losses = train_batch(batch, model, \n",
        "                                  optimizer, device)\n",
        "            # Record the current train loss.\n",
        "            pos = epoch + (ix + 1) / N\n",
        "            log.record(pos = pos, trn_loss = loss.item(), \n",
        "                       end = \"\\r\")\n",
        "        if test_loader is not None:\n",
        "            N = len(test_loader)\n",
        "            for ix, batch in enumerate(test_loader):\n",
        "                loss, losses = validate_batch(batch, model, \n",
        "                                         optimizer, device)\n",
        "                \n",
        "                # Record the current validation loss.\n",
        "                pos = epoch + (ix + 1) / N\n",
        "                log.record(pos = pos, val_loss = loss.item(), \n",
        "                           end = \"\\r\")\n",
        "    log.report_avgs(epoch + 1)\n",
        "    return log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "70b3f32f5f074f0987f9401e06f2584d",
            "e41b2c2eb77840a7ab3fe54ed259500e",
            "a20abefeac594ed9935a6630d840f438",
            "c3be8af1b006447d8339420840c022bc",
            "b328bd80c0604fa9b573dc0d244a20dd",
            "3bccf7fb570f472abdfb025a8d3345af",
            "7bb45643b4014a529c55b29b20928fc3",
            "cba7d027f51f4ba7b872ab76f90ffda0",
            "01bda21fb9ad4f259253084c7124d3df",
            "b3a853b72b484e2cab17a0fab8ba2b1b",
            "9db280c34f3b465eb06f14906f73302f"
          ]
        },
        "id": "1oOSbLOJn8U9",
        "outputId": "d5fef086-a023-4abd-f36c-9c5cc0626570"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70b3f32f5f074f0987f9401e06f2584d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 10.000  val_loss: 0.211  trn_loss: 0.182  (11478.54s - 0.00s remaining)\n"
          ]
        }
      ],
      "source": [
        "# Create the faster rcnn model with 5 classes - satellite-dish, router, usb-stick, server-rack and \n",
        "# background.\n",
        "model = get_object_detection_model(num_classes = 7,   \n",
        "                        feature_extraction = False)\n",
        "# Use the stochastic gradient descent optimizer.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                        lr = 0.005, \n",
        "                    momentum = 0.9, \n",
        "             weight_decay = 0.0005)\n",
        "# Train the model over 1 epoch.\n",
        "log = train_fasterrcnn(model = model, \n",
        "               optimizer = optimizer, \n",
        "                        n_epochs = 10,\n",
        "             train_loader = train_dl, \n",
        "                test_loader = val_dl,\n",
        "             log = None, keys = None,\n",
        "                     device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MWKoLyFMx8Uv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save to file in the current working directory\n",
        "pkl_filename = \"fasterrcnn_model2.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70b3f32f5f074f0987f9401e06f2584d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e41b2c2eb77840a7ab3fe54ed259500e",
              "IPY_MODEL_a20abefeac594ed9935a6630d840f438",
              "IPY_MODEL_c3be8af1b006447d8339420840c022bc"
            ],
            "layout": "IPY_MODEL_b328bd80c0604fa9b573dc0d244a20dd"
          }
        },
        "e41b2c2eb77840a7ab3fe54ed259500e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bccf7fb570f472abdfb025a8d3345af",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb45643b4014a529c55b29b20928fc3",
            "value": "100%"
          }
        },
        "a20abefeac594ed9935a6630d840f438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cba7d027f51f4ba7b872ab76f90ffda0",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01bda21fb9ad4f259253084c7124d3df",
            "value": 167502836
          }
        },
        "c3be8af1b006447d8339420840c022bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a853b72b484e2cab17a0fab8ba2b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_9db280c34f3b465eb06f14906f73302f",
            "value": " 160M/160M [00:00&lt;00:00, 191MB/s]"
          }
        },
        "b328bd80c0604fa9b573dc0d244a20dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bccf7fb570f472abdfb025a8d3345af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb45643b4014a529c55b29b20928fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cba7d027f51f4ba7b872ab76f90ffda0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01bda21fb9ad4f259253084c7124d3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3a853b72b484e2cab17a0fab8ba2b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db280c34f3b465eb06f14906f73302f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}